> A crossâ€‘platform Flutter app providing visionâ€‘enabled accessibility features  
> for visually impaired users and their caregivers.

![Flutter Version](https://img.shields.io/badge/flutter-3.7.2-blue)  
![Dart Version](https://img.shields.io/badge/dart-3.1.5-blue)  
![License](https://img.shields.io/badge/license-MIT-green)

---

## ğŸš€ Features

- **Onboarding**  
  Animated, fullâ€‘screen walkthrough with highâ€‘contrast branding.

- **Home Dashboard**  
  Dynamic greeting, quickâ€‘action carousel, recent activity, and caretaker panel.

- **Text Reader**  
  Live camera capture (mobile & web), OCR via Azure GPT, TTS playback with play/stop controls.

- **Scene Description**  
  Describe surroundings, identify objects & colors via ML Kit.

- **Navigation Aid**  
  Voiceâ€‘guided indoor/outdoor routing with obstacle alerts.

- **Face Recognition**  
  Detect & name familiar faces via ML Kit Face Detection.

- **Emergency SOS**  
  Oneâ€‘tap alert calls & location sharing with paired caregiver.

- **Video Call**  
  Twoâ€‘way RTC video chat between user and caregiver.

- **ESP32 Cam Integration**  
  Optional external camera support for arduinoâ€‘powered wearables.

---

## ğŸ“¦ Tech Stack & Dependencies

- **Flutter** (â‰¥â€¯3.7.2) & **Dart** (â‰¥â€¯3.1.5)  
- **State Management**: Provider  
- **OCR & Vision**:  
  - Google ML Kit (Text, Face, Object)  
  - Azure OpenAI GPTâ€‘4 Vision endpoint  
- **TTS**: `flutter_tts`  
- **Camera**: `camera` + Web `<video>` via `dart:html`  
- **Networking**: `http`, `connectivity_plus`  
- **Local Storage**: `shared_preferences`  
- **Mapping**: `flutter_map` + `latlong2`  
- **WebRTC**: `flutter_webrtc`  
- **Others**: `permission_handler`, `image`, `logger`, `url_launcher`

---

## âš™ï¸ Getting Started

### 1. Clone the repo

git clone https://github.com/Twenethomas/aniwasmartlens.git
cd aniwasmartlens2. Configure environment
Copy the example env file and fill in your keys:

bash
Copy
Edit
cp .env.example .env
Edit .env and set:

dotenv
Copy
Edit
AZURE_OPENAI_ENDPOINT=https://buzznewwithgpt4.openai.azure.com
AZURE_OPENAI_KEY=YOUR_API_KEY
AZURE_COGNITIVE_SERVICES_KEY=YOUR_COGNITIVE_KEY
3. Install dependencies
bash
Copy
Edit
flutter pub get
4. Run the app
Mobile (Android/iOS):

bash
Copy
Edit
flutter run
Web:

bash
Copy
Edit
flutter run -d chrome
ğŸ§© Project Structure
perl
Copy
Edit
lib/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ services/       # OCR, networking, history, state
â”‚   â””â”€â”€ utils/          # image preprocessing, token trimming
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ onboarding/     # Onboarding screens & widgets
â”‚   â”œâ”€â”€ home/           # Dashboard & widgets
â”‚   â”œâ”€â”€ text_reader/    # Capture, OCR & TTS UI
â”‚   â”œâ”€â”€ scene_description/
â”‚   â”œâ”€â”€ navigation/
â”‚   â”œâ”€â”€ face_recognition/
â”‚   â”œâ”€â”€ emergency/
â”‚   â”œâ”€â”€ video_call/
â”‚   â””â”€â”€ esp32_cam/      # (stub for future)
â”œâ”€â”€ state/              # AppState & Provider setup
â””â”€â”€ widgets/            # Shared UI components
ğŸ“– Usage
On first launch, swipe through onboarding slides.

From Home, tap any feature card to launch.

Text Reader: capture or pick an image â†’ autoâ€‘compress & send to Azure OCR â†’ view & speak text.

History: view past readings in the History screen.

Emergency: quickly alert your paired caregiver.

ğŸ¤ Contributing
Fork the repository

Create a feature branch (git checkout -b feature/YourFeature)

Commit your changes (git commit -m 'Add some feature')

Push to the branch (git push origin feature/YourFeature)

Open a Pull Request

Please adhere to the existing code style and include relevant tests if possible.

ğŸ“ License
This project is licensed under the MIT License.

ğŸ“ Support
If you run into issues or have questions, please file an issue on GitHub or email support@aniwasmartlens.org.

Built with â¤ï¸ by the AniwaSmartLens team
