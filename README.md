# ğŸ‘ï¸ Assist Lens: Your Hands-Free AI Companion

**Assist Lens** is a mobile app designed to help **visually impaired people** live more independently. Using **real-time AI**, **voice control**, and **smart computer vision**, it serves as a hands-free companion.  
Just say: _**â€œHey Assist Lensâ€**_ to begin.

---

## ğŸŒŸ Features

| ğŸ”¹ Feature | ğŸ“ Description |
|-----------|----------------|
| ğŸ§  **Aniwa AI Chat** | Talk to **Aniwa**, your AI assistant powered by **Gemini AI**. Ask anything, get smart, human-like responses. |
| ğŸ™ï¸ **Voice Control** | Fully voice-enabled. Control everything with simple voice commandsâ€”no touch needed. |
| ğŸ“– **Text Reader** | Point the camera at any printed material (signs, books, menus) and the app reads it aloud. It can auto-correct recognition errors and translate to English. |
| ğŸŒ„ **Scene Description** | Take a photo and get a natural-language description of whatâ€™s in the image. E.g., _â€œA park with trees and two people walking.â€_ |
| ğŸ§  **Object Detection** | Recognizes everyday objects in real-time (e.g., â€œchair,â€ â€œdog,â€ â€œbottleâ€). |
| ğŸ§‘â€ğŸ¤â€ğŸ§‘ **Face Recognition** | Identifies familiar faces using your device camera. |
| ğŸ—ºï¸ **Smart Navigation** | Get voice directions, object distance alerts, and destination notifications using maps and sensors. |
| ğŸš¨ **Emergency Help** | Quickly alert emergency contacts and share your live GPS location with one tap or command. |
| ğŸ•“ **Activity History** | Access logs of past chats, AI descriptions, and user interactions. |
| ğŸŒ— **Day & Night Mode** | Adaptive themes for daylight and low-light conditionsâ€”automatically or manually switch. |

---

## ğŸ› ï¸ Technology Stack

| ğŸ§© Technology | ğŸ”§ Description |
|--------------|----------------|
| ğŸ’™ **Flutter** | Cross-platform mobile framework used for building the app. |
| ğŸª„ **Provider** | Lightweight, reactive state management. |
| ğŸ” **Google ML Kit** | On-device OCR, face detection, and object recognition. |
| ğŸ§  **Gemini API** | Conversational AI from Google powering Aniwa. |
| ğŸ—£ï¸ **Speech-to-Text (STT)** | Converts user speech into actionable commands. |
| ğŸ”Š **Text-to-Speech (TTS)** | Reads back AI responses, recognized text, or scene descriptions. |
| ğŸ§ **Picovoice Porcupine** | Lightweight wake-word engine for always-on listening. |
| ğŸ“ **Geolocator + Flutter Map** | Real-time location tracking and smart voice navigation. |
| ğŸ“· **Flutter Camera & Permissions** | Camera integration for vision features. |
| â˜ï¸ **Firebase** | Backend services including real-time database and cloud functions. |

---
## ğŸ“‚ Project Structure
```

```markdown
assist\_lens/
â”œâ”€â”€ android/                   # Android-specific code
â”œâ”€â”€ assets/                    # Images, fonts, AI models
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ routing/           # Navigation between screens
â”‚   â”‚   â””â”€â”€ services/          # Speech, AI, network helpers
â”‚   â”œâ”€â”€ features/              # Core features (chat, camera, etc.)
â”‚   â”‚   â”œâ”€â”€ aniwa\_chat/
â”‚   â”‚   â”œâ”€â”€ pc\_cam/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ main.dart              # App entry point
â”œâ”€â”€ pubspec.yaml               # Project dependencies
â””â”€â”€ README.md                  # This file

```
## ğŸš€ Getting Started

### âœ… Prerequisites

- [Flutter SDK](https://docs.flutter.dev/get-started/install) (v3.7.2 or newer)
- VS Code or Android Studio (with Flutter/Dart plugins)
- JDK 11+
- Android/iOS device or emulator

### ğŸ“¥ Installation

1. Clone the repo:

```bash
git clone https://github.com/yTwenethomas/aniwasmartlens.git
cd assist_lens
````

2. Install dependencies:

```bash
flutter pub get
```

3. Set up Firebase:

* Create a project in [Firebase Console](https://console.firebase.google.com/)
* Add Android/iOS app
* Download:

  * `google-services.json` â†’ `android/app/`
  * `GoogleService-Info.plist` â†’ `ios/Runner/`

4. Fill in your Firebase details in `lib/main.dart`:

```dart
await Firebase.initializeApp(
  name: 'assist_lens',
  options: const FirebaseOptions(
    apiKey: 'YOUR_API_KEY',
    appId: 'YOUR_APP_ID',
    messagingSenderId: 'YOUR_MESSAGING_SENDER_ID',
    projectId: 'YOUR_PROJECT_ID',
  ),
);
```

5. Add Gemini API Key in `lib/core/services/gemini_service.dart`:

```dart
const apiKey = "YOUR_GEMINI_API_KEY";
```

6. Add AI model files (`.tflite`, `.txt`) in `assets/ml/`.

---

## ğŸ¤– Android Setup

1. Add permissions to `android/app/src/main/AndroidManifest.xml`:

```xml
<uses-permission android:name="android.permission.RECORD_AUDIO" />
<uses-permission android:name="android.permission.CAMERA" />
<uses-permission android:name="android.permission.FOREGROUND_SERVICE_MICROPHONE" />
```

2. Define the voice service:

```xml
<application>
    <service
        android:name=".VoiceAssistantService"
        android:foregroundServiceType="microphone" />
</application>
```

3. In `android/app/build.gradle`:

```gradle
android {
    defaultConfig {
        minSdk = 21
    }
}
dependencies {
    implementation 'io.flutter.plugins.camera:camera-camera2:0.11.1'
}
```

4. Check `VoiceAssistantService.kt` (path: `android/app/src/main/kotlin/...`):

```kotlin
package com.example.assist_lens
```

Update the package name if different.

---

## ğŸ iOS Setup

1. Add permission descriptions in `ios/Runner/Info.plist`.

2. In Xcode, go to **Signing & Capabilities** > enable:

* Background Modes

  * âœ… Audio, AirPlay, Picture in Picture
  * âœ… Voice over IP

---

## â–¶ï¸ Run the App

```bash
flutter clean
flutter pub get
flutter run
```

---

## ğŸ—£ï¸ Using the App

* Say **â€œHey Assist Lensâ€** to activate the assistant.

* Use commands like:

  * â€œDescribe the scene.â€
  * â€œRead this text.â€
  * â€œWhoâ€™s around me?â€
  * â€œCall emergency.â€

* Tap the **Chat** tab to chat with Aniwa.

* Use the **Explore** tab for manual features.

---

## ğŸ¤ Contributing

We welcome contributions!

1. Fork the repo
2. Create your feature branch: `git checkout -b feature/your-feature`
3. Commit: `git commit -m "Added new feature"`
4. Push: `git push origin feature/your-feature`
5. Open a **Pull Request**

Try to follow existing code style and naming conventions.

---

## ğŸ“„ License

This project is licensed under the MIT License. See [`LICENSE.md`](./LICENSE.md) for details.

---

**Built with ğŸ’™ for the visually impaired community.**

